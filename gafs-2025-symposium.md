# [Opening Ceremony] Global AI Frontiers Symposium 2025 | Yann LeCun | Yejin Cho

Unknown Speaker:
The AI we face today. The source of this powerful technology and a monumental in making advancement requiring vast capital and resources.

We must move beyond the risks of concentrated privilege by building trust through international cooperation. From fundamental research to real world applications. We move forward, together, toward a future of AI that is fair and safe.

Amid intense debate, the standards for AI's future are taking shape. Fairness and accountability in algorithms. Autonomy in physical AI robots. Cutting edge innovations transforming human health and life.

At the heart of every challenge and discussion stand the National AI Research Lab and the Global AI Frontier Lab. In partnership with the world's leading minds, we are building a global network, a nexus of AI innovation,

 where technological progress meets social responsibility. Today, at this symposium, we share our achievements and vision, boldly venturing into AI's uncharted frontier,  beyond the limits of imagination.

A new era of international cooperation begins now.

Speaker 1:
A new era of international cooperation for AI. So we just opened today's symposium by sharing our theme video. Distinguished guests, ladies and gentlemen, good morning and welcome to the Global AI Frontiers Symposium 2025.

 Before we begin, please allow me to introduce myself. My name is Seojin Jessica Park, and it is my great honor and privilege to serve as your host. For today's meaningful symposium. 안녕하십니까 여러분. Thank you very much. First and foremost,

 I would like to extend my sincere gratitude to all of you for joining us here today out of your busy schedules. This symposium is being held under the theme, Collaborating for the Innovation in AI.

It brings together researchers from the National AI Research Lab and the Global AI Frontier Lab to promote international collaboration and exchange.

Throughout today's sessions, we will be able to explore not only the advancement of AI technology,  but also its wider social impact, as well as its responsibilities for the future.

In addition, the Global AI Frontiers Symposium 2025 is being live-streamed globally on our YouTube channel. And we are truly delighted to have this opportunity to share knowledge and insights not only here with all of you,

 but also with our global audiences joining us online. As we kick off the event, I would like to take a moment to introduce our distinguished guests who have joined us today. And please welcome each of them with a warm round of applause.

First of all, we are honored to be joined by Kyung Hoon Bae,  Deputy Prime Minister and Minister of Science and ICT. Please stand up and welcome it with a big round of applause welcome and Next,  we are pleased to welcome Hyung Doo Choi.

Member of the National Assembly. Good morning and welcome. And we're also delighted to have Yejin Cho, professor at Stanford University. Good morning. Please stand up and say hello to our audience. Thank you.

Joining us as well is Yann LeCun, co-director of the Global AI Frontier Lab. Good morning. And we are also joined by Mr. Kyunghyun Cho, co-director of the Global AI Frontier Lab. Good morning.

Next, please welcome Juan de Pablo, executive vice president at New York University. Good morning, sir. And next we are also pleased to have Jinbae Hong, President of IITB. Good morning and welcome.

And joining us as well is Jinsu Lee, Director General at the Ministry of Science and ICT. Good morning. And we're also honored to welcome Joon Hyung Lee, Director General at the Seoul Metropolitan Government. Good morning.

And also, last but not least, please welcome Kim, director of the National AI Research Lab. Good morning. Thank you so much. Today, actually, we're joined by many respected guests and scholars from near and far.

While time doesn't allow for individual introductions, please know that your presence makes this event even more special. And moving forward, it is now time for the remarks. And this opening remarks allow me to invite Mr.

Kyung Hoon Bae, Deputy Prime Minister and Minister of Science and ICT. Please welcome him with a big round of applause.

Speaker 2:
Good morning, distinguished guests and ladies and gentlemen. I'm Kyung-Eun Bae, Deputy Prime Minister and Minister of Science and ICT.

I extend my deepest gratitude to our distinguished guests and researchers who have honored us with their presence at today's Global AI Frontier Symposium. First, I would like to thank Mr.

Choi Young-doo, I'm a member of the National Assembly. And Dr. Juan de Pablo, Executive Vice President for Global Science and Technology, NYU, for joining us despite their busy schedule. Thank you.

I also extend my special thanks to Professor Yann LeCun and Professor Choi Yejin,  Professor Cho Kyunghyun, for traveling long distance to be with us. Thank you.

We now stand at a monumental turning point where AI is reshaping the landscape of science,  technology, industry, and every sector of society.

The government of Republic of Korea is laying the groundwork for a nationwide AI transformation to drive Korea's genuine growth and a new leap forward.

We aimed to develop the world-class AI foundation models, process specialized AI models, and realize AI for all.

To achieve this, we plan to shift with secure clearing edge GPU resources and establish ultra-high performance network and high quality data infrastructures. We will also actively support the development of next-generation technologies.

국제적으로 개발된 MPU를 포함한 AGI and physical AI technologies. In addition, we are also striving to enhance our competitiveness in AI technologies through international cooperation. Establishing the National AI Research Lab in Korea,

 we are building a collaborative network with researchers from around the world and nurturing the next generation of global leaders through joint international research. At the same time, we are actively supporting our researchers.

In collaborating closely We did a world top-tier research team and achieving world-class research outcomes by establishing the Global AI Frontier Lab in the United States,  the epicenter of global AI research.

Today's symposium marks the first international event jointly hosted by these two key research labs,  providing a platform for researchers to explore the present and future of AI.

Distinguished researchers, I hope this symposium will open new horizons for AI research and collaboration.

The Ministry of Science and ICT will continue to strengthen international cooperation in AI research and actively support the creation of global ecosystem for AI Research Collaborations.

Once again, I sincerely thank all of you for joining us today,  and I wish you continued success and good health. Thank you.

Speaker 1:
Thank you Minister Bae for your inspiring remarks.

So Minister Bae shared the significance of this event in the era of AI transformation and the shared vision of the government to foster international cooperation in the field of artificial intelligence.

Let's give him a big round of applause once again. Thank you very much. Next, we will have the congratulatory remarks by Hyung Doo Choi,  member of the National Assembly from the Science ICT Broadcasting and Communications Committee.

Let's welcome him with a big round of applause.

Unknown Speaker:
Thank you. On behalf of National Assembly on AI, ICT and Science Technology Committee,  I really welcome All the distinguished experts and scholars from the world-renowned research institutions.

We are now at the Korean National Assembly and our committee is now in the middle of a national audit. So that's why I'm here by myself. I would like to convey our sincere thanks to all of the participants of this great event,

 representing my whole colleague, 20 members of our committee. From now on, I'll take advantage of the new technology of Blitto, our AI company. 반갑습니다. 저는 국회과학기술정보방송통신위원회 야당 간사 최용두입니다.

In English, ranking member of the congress or vice chair of the president. Especially, I'm in charge of the AI. Science Technology Subcommittee. So I'm chairperson of the subcommittee as well.

Speaker 3:
Yeah.

Unknown Speaker:
It's a shame that our colleagues couldn't be with us today during the national inspection period. But on behalf of them, I would like to say thank you and reflect the contents of today's presentation and testimony to our National Assembly.

Korea, which is related to AI, has the opportunity to be the first in the world to move forward in 2010.  The moment is April 2010. Looking at the Great Wall of China, the leaders and generals of China were surprised,

 and the United States, which was the country of AlphaGo, was of course ahead. At that time, the place where it happened, the moment of Eureka, was Seoul. But we missed that moment. In particular, politicians need to reflect a lot.

It's a bit late to say this, but although the political power is in the midst of many political disputes and political conflicts,  we are truly serious about AI. AI is receiving a huge amount of support.

And next year's budget, including the physical AI budget,  will be calculated and invested in the largest amount compared to our budget. Although there are many cases of political disputes due to various opinions, we,

 the National Assembly, have decided to lead Korea and the world in this civilizational and political transition period through AI. I would like to convey to the National Assembly the many topics that have been discussed in today's debate,

 and I hope that the experts, scholars, and many innovators of AI companies 노력과 아이디어가 대한민국과 전세계의 AI 발전을 이끄는데 기여할 수 있도록 저희들 힘을 부태겠습니다. 감사합니다.

Speaker 1:
Thank you very much Mr. Choi for your inspiring words. He extended his congratulatory remarks on behalf of the National Assembly and shared the government's commitment for AI. Let's give him a big round of applause once again.

Up next, we will now take a look at a video message sent by Ms. Tonga Huang, member of the National Assembly from the Science, ICT, Broadcasting, and Communications Committee. She was unable to join us in person due to a prior commitment,

 but she has sent us a video message. So, let's have a look.

Unknown Speaker:
Hello, I'm Hwang Jung-ah, a member of the National Assembly of the Korean Federation of the Democratic Party of Korea.

Congratulations on the successful replacement of the AI Frontier International Symposium 2025. I would like to thank Baek Young-hoon,

 the head of the Science and Technology Information Communication Department, and Hong Jin-bae, the director of IITP. I would like to express my deepest gratitude to all the guests,  experts who have prepared the presentation and discussion,

 and to all the workers who have worked hard to prepare for the event. Today, we have professors, researchers, students,

 and industry officials from all over the country gathered together to discuss the future of AI and share a vision for cooperation. 지금까지 과학기술은 경쟁을 통해 진보하고 협력을 통해 확장되어 왔습니다. AI 혁신도 마찬가지입니다.

창문의 경계와 국경을 넘는 글로벌 협력은 우리 AI 기술의 발전에 속도와 깊이를 더해줄 것입니다. 이런 흐름 속에서 국가 AI 연구 거점과 글로벌 AI 프론티어 랩은 국내 대학과 기업뿐 아니라 해외 유수 연구기관과의 협력을 통해 인재를 양성하고 있습니다. 또 산업과 연구의 가교 역할 역시 훌륭히 해내며 대한민국이 글로벌 AI 생태계 중심으로 힘차게 도약하고 있음을 잘 보여주고 있습니다.

I will do my best to build a legal and institutional foundation so that the National Assembly can listen to the voice of the field and protect the golden time of the three major AI initiatives.

I hope that this symposium will be a turning point in the AI innovation that connects research,  industry, and international cooperation. Once again, I would like to thank all of you for your passion and dedication.

I hope you will be a reliable center of this journey for Korea to become one of the three major AI powers. I will always support you. Thank you.

Speaker 1:
Thank you so much for the warm messages. As she said, we are really at the turning point of AI innovation,  and I truly believe that this symposium will be the platform for cooperation beyond the limits and borders.

Then we will move on to the welcoming remarks. And for that, I would like to invite Mr. Jinbae Hong, President of the Institute for ICT Planning and Evaluation. Let's welcome him with a big round of applause.

Unknown Speaker:
Good morning. Distinguished guests, ladies and gentlemen, I'm Jinbae Hong, the president of IITP, the Institute for ICT Planning and Evaluation.

It's my great honor and pleasure to welcome all of you distinguished scholars and guests from Korea and around the world. To the Global AI Frontiers Symposium 2025. First,

 I'd like to express my sincere gratitude to Deputy Prime Minister and Minister of Science and ICT Kyung Hoon Bae for joining us today despite his busy schedule.

And my special thanks go to Assemblyman Hyung-Doo Choi and to Assemblywoman Jung-Ah Hwang. And I also warmly welcome our distinguished guests from abroad, including Executive Vice President Juan Pablo,

 and Professors Yann LeCun and Kyung Hyun Jo,  of New York University and Professor Yejin Cho of Stanford University for joining us today. Last year, the Ministry of Science and IST and IITP launched two major AI research labs.

The AI Research Hub in Seoul And the Global AI Frontier Lab in New York as a foundation. for global AI research collaboration and talent exchange.

Through these two AI research labs, outstanding researchers from both countries have been conducting vibrant joint research.

At the AI Research Lab in Seoul, researchers are collaborating with experts from 13 international institutions on topics such as transcending neural scaling laws and robotic foundation models.

Researchers at the Global AI Frontier Lab in New York. Our advancing studies in embodied AI as well as robust, responsible,  and trustworthy AI based on close cooperation between Korea and the U.S. 이 symposium은 이 자료를 공유하기 위해 진행되었습니다.

Two AI research labs. Strengths in communication and forces synergy through open collaboration. When different perspectives converge, they open new possibilities and lead to discoveries we could never have imagined alone.

As we learn and explore together, knowledge crosses borders and we move forward Hand in hand toward the future of AI. IITP will continue to be your strong partner in advancing AI research and innovation.

Once again, I send my sincere appreciation. Thank you to all of you for joining us today. 여러분께 진심으로 의미 있는 And inspiring symposium. Thank you.

Speaker 1:
Thank you so much for the thought-provoking remarks. So he introduced the two major research labs and as he said by working together we'll be able to create synergy. Thank you very much and let's give him a big round of applause once again.

And now it is a very exciting moment. We will now take a look at congratulatory video messages from world-renowned AI scholars,

 including Professor Geoffrey Hinton and Yoshua Bengio, as well as other distinguished researchers from around the world. So please take a look at it.

Unknown Speaker:
First, I'd like to congratulate the symposium attendees and the Korean government on being at the forefront of research on AI. Some critics think that AI is mostly hype and will soon run out of steam or maybe electricity.

I sometimes wish they were right because then we would not have to face all the dangers that come from bad actors misusing highly intelligent AI or from AI itself becoming a bad actor. But progress in AI is still very rapid.

Even if progress is only linear,  the AI we have in 10 years' time will make the AI we have now look as primitive as the chatbots we had in 2015.

Other critics say we should deliberately slow down the development of AI so that we have time to figure out how to make it safe. This might be sensible, but it's not going to happen.

There are too many good uses of AI in healthcare, in education, or for designing new drugs and materials. Also, if one nation slows down, others will not.

My own belief is that we need public pressure on politicians everywhere to force them to impose regulations.

The public can easily understand that AI companies should not be allowed to release untested chatbots or advanced AI agents that could collaborate with each other in ways we do not understand.

As for the threat of AI itself becoming a bad actor,  recent research by Anthropic has made it very clear that this threat is real. An AI will cheat and lie and even resort to blackmail in order to survive.

We urgently need to find a way to allow humans to coexist with the super-intelligent AI that most experts think will be here within 20 years.

The one piece of good news is that sensible democratic countries that have rid themselves of populist crooks will collaborate on this research because their interests are aligned.

No country wants AI to take over, and if any country figured out how to prevent this happening,  it will gladly share what it had learned with other countries. I wish you all good luck for the future and hope you enjoy the symposium.

Speaker 3:
Thank you to the International Conference on AI Frontiers 2025 for this opportunity to say a few words today.

As highlighted in the Seoul Declaration in 2024, AI has the potential to bring tremendous benefits and help us address some of our time's most pressing needs,  but only if we steer it wisely. The reason for this is simple.

Intelligence and knowledge give power. That power can be used for the public good or it can be concentrated in a few hands or misused for malicious purposes. Models, capabilities, and agency continue to advance rapidly and consistently.

If scientifically observed trends continue, autonomous agents will surpass most humans across most cognitive tasks in maybe as little as 5 to 10 years.

This would be a radical change in the history of humankind with the potential to greatly enhance our well-being,  but also, if we're not careful, to introduce a series of major risks for human dignity,

 democracy, geopolitical stability, and even the future of humanity. Hence, we must collectively work on both policy, with national laws and international treaties, and science.

We need to better understand how to design AI that will be both highly capable and will not harm people,  and make sure we can always maintain control of AI models so that they behave in really safe ways.

I have recently launched a new non-profit organization called Law Zero to tackle these issues,  but far more efforts are needed. Korea has many strengths and opportunities to help forge a better path for the future with AI.

Great research universities, a great talent pool in AI, very strong industrial research in AI,  both on algorithms and hardware, especially around machine learning,  which is at the heart of today's AI.

Korea is also a leader in high-resolution AI chips that will matter more and more in the future.

I would like to conclude by sharing something that was highlighted in the first version of the International AI Safety Report that I have been sharing. It says the following, the future of general purpose AI is uncertain.

Both very positive and very negative outcomes are possible. Much depends on how societies and governments will act. Thank you.

Speaker 4:
Collaborative research beyond borders is of crucial importance to advance science and to achieve impactful results.

Speaker 5:
This partnership holds immense potential for the future of artificial intelligence research and innovation.

Speaker 4:
And collaborative research beyond borders is the key to make an impact.

Speaker 1:
So I look forward to the possibilities this partnership will bring.

Speaker 3:
I believe that collaborative research beyond boundaries is the key to advancing Technological frontiers as well as achieving impactful results through a robust exchange of knowledge and expertise.

Unknown Speaker:
Together,  I believe we can tackle some of the most pressing challenges in AI to help cutting-edge solutions that mutually benefit our societies.

I look forward to the possibilities this partnership will offer and to the future we will build together in the field of artificial intelligence. Together, let's lead the way to AI innovation.

Speaker 1:
Thank you so much. How amazing it is to see all those world-renowned experts and scholars in one video. Thank you once again for the thoughtful messages and hoping to see you in person in the near future.

Up next, we will have a group photo session to commemorate today's event. And for that, we kindly ask all guests seated in the first row to please stand up and come forward to the stage.

And although many distinguished guests are in attendance today,  we kindly ask for your understanding that due to the limited space on the stage,  not everyone will be able to join in the group photo.

And we would like to express our sincere appreciation to all of you for joining us today. Take one step to your left hand side. Yes. And please stand in between the shoulders so that we can see your face. Yes. 네, 좋습니다. Please look at the camera.

And could you come a little bit closer to the camera? Yes. Okay. And let's take one more photo with a fighting pose all together. So I'll be... I'll be saying one, two, three, fighting! Finger hearts. Yes. Cross your thumb and finger together.

네, 감사합니다. Thank you very much. Now you can return to your seats. Once again, thank you very much for your cooperation. 자리로 돌아가셔서 착석 부탁드리겠습니다. And after a brief moment to rearrange the stage, we will start right away with our keynote speech.

So please remain seated. Thank you. And now, we shall start the most awaited part of today's event. And we will now have the keynote speech. For that, I would like to invite Professor Yann LeCun, co-director of the Global AI Frontier Lab,

 and he will be talking about the training world models. Let's welcome him with a big round of applause.

Speaker 5:
Thank you very much. A real pleasure to be here and a real honor in the presence of all the distinguished guests,  some of whom I met last time I visited Korea in December.

So what I'm going to try to tell you about is the next revolution in AI. We've seen a number of revolutions in AI over the last 15 years. Deep learning sort of came to the fore a dozen years ago or so. More recently, LLMs and chatbots.

But we're not done yet. A couple more revolutions before we get to truly intelligent machines. And in my opinion, as I've been saying for about 10 years now,  I think it will come through this concept of world models.

So I'm going to tell you a little bit about the motivation behind this and how we do this and what progress we've made over the last few years towards that. So first of all, Why do we need AI systems that have human-level AI or more?

And the reason is, in the future, maybe not too distant,  all of us will be kind of working around with wearable devices that will incorporate AI assistance that will help us in our daily lives.

But currently, The technology is really not completely ready for that. I'm actually wearing a pair of smart glasses right now. I can take a picture of you, by the way. Smile. All right.

I could have asked my assistant to actually take a picture of you as well. But we need those systems to have human-like intelligence if we want the interaction with them to be easy for people to To understand.

And we need those systems to help us in our daily lives. We want them to some extent to be... smarter than us in many ways. Many of us here are leaders in academia or the world of politics or business,

 and we're familiar with working with people who are smarter than us. I certainly am very familiar with the concept of working with people who are smarter than me. In fact, my entire job as a professor is to make my students smarter than me.

It's a wonderful thing to have a staff of people,  whether they are real people or virtual people who are smarter than you. It just makes you, it empowers you. So that's what our relationship with future AI systems will be.

They might be smarter than you, but they'll work with us, they'll work for us. But we need them to be understanding our world, et cetera. We're not there yet.

The technology we have is really not that good, and I hope you will pardon my French here. Current AI architectures really suck, in a way. They're nowhere near the kind of capabilities that we observe in humans and animals.

Human and animals have mental models of the world. Their behavior is driven by objectives or drives,  if you want to call them this way,  and they can reason and plan complex action sequences.

And we don't give enough credit to animals as to how smart they are. They really are smart. They're smarter than the best AI systems that we currently have.

What we need really is, you know, a pass to what I call AMI, Advanced Machine Intelligence.

I don't like the phrase AGI that a lot of people have employed because the G means general and that implies that human intelligence is general and human intelligence is not general at all. We're very specialized.

In fact, we know we're specialized. In fact, a demonstration of this was shown here in Korea 10 years ago with AlphaGo. The experiments of this type show that AI systems can be superior to humans in a number of narrow ways.

And that shows that humans are really not that general. So the concept of general intelligence is nonsense. But advanced machine intelligence, something that maybe would match human intelligence, is a good concept.

So we need systems that understand the physical world. And currently, our technology really does not. We need systems that have persistent memory. We need systems that can plan.

Complex action sequences, systems that can reason, have long chains of reasoning,  and we need systems that are controllable and safe. And current AI architectures have none of this, essentially, or only to a very small extent.

So perhaps we can get inspiration from human learning and look at how babies can learn how the world works. When we are little, a few months old, we build a mental model of how the world works.

And we do this just by basically observing the world initially and then later by interacting with the world. But we learn basic concepts like intuitive physics,  like the notion that objects that are not supported fall because of gravity.

We learn this around the age of nine months. It takes a long time. And so, again, how do we do this with machines? How do we get them to learn how the world works by observation?

And current architectures clearly don't do it because although we have systems that can,  you know, pass the bar exam or solve mathematics problem and write code, we still don't have domestic robots.

We still don't have completely autonomous level 5 self-driving cars. And the ones we have require a lot more than the 20 hours of practice that most teenagers would require to learn to drive.

We don't have domestic robots to do tasks that a 10-year-old can do the first time we ask them without any learning,  basically what we call zero shot. In fact, our AI systems, in terms of their understanding of the physical world,

 are not nearly as smart as a house cat. So that's the gap we need to fill. And this is a very old issue in AI. It's called the Moravec paradox, which, you know, some tasks that Humans consider complex like playing go or chess.

or planning a path or something like this are considered complicated for humans but seem to be fairly simple for computers. But conversely, things that many animals can do effortlessly, we can't reproduce yet with machines.

So the future of AI, I think, is more physical than the current one. And perhaps it's explained by this very simple calculation.

A typical LLM, large language model, is trained with on the order of three Times 10 to the 13 tokens, 30 trillion tokens. The token is typically three bytes, so the total amount of data for pre-trading is about 10 to the 14 bytes.

This is all the text that's publicly available on the internet, basically. It would take any of us on the order of 400 to 500,000 years to read. Now, compare this with what a human child has seen in the first four years of life.

The first four years of life correspond to about 16,000 hours of wait time. And we have 2 million optical nerve fibers getting to your brain, each carrying about 1 byte per second. Do the arithmetics, and that's about 10 to the 14 bytes.

So a 4-year-old has seen more data than the biggest LLMs,  but through high-bandwidth sensory input like vision or touch or audition and other senses. And so that tells you we're never going to get to human-level AI by just training on text.

Nous avons besoin des systèmes d'IA aussi. and train themselves through sensory input like video. There's a second issue,

 which is that we need AI systems to be able to produce the output through some sort of reasoning or some sort of search,  not merely through a propagation through a fixed number of layers of a neural net.

So LLMs and other systems basically compute the output by propagating a signal through a fixed number of layers. of a neural net and then producing an output. But really the way we humans and animals,

 many animals reason is by thinking about something,  but sort of manipulating a mental model and then looking for,  searching for a sequence of actions that will accomplish a particular task that we set out to do.

And so this is inference by optimization, not inference by forward propagation. And that's a very different concept that LLMs do not implement at the moment or only in very trivial fashion. So, you know, LLMs predict autoregressively.

Feed them a sequence of discrete tokens, which may represent text or other things,  and then you have them produce the next token, then you shift that into the input,  and then produce the second next token, et cetera.

There is some very basic fatal flaw with this. First of all, it is inferenced by forward propagation,  where the amount of information that's communicated from one step to the next is reduced to basically one token,

 which is three bytes, typically. But there is the issue of In fact, it's one of your papers, Jiejun, isn't it? Choi, right here. Where you can show that this autoregressive process will diverge at some point.

And there is really no way to fix this within the current paradigm. So one thing I've been interested in for a long time, over 10 years, well actually 10 years,

 is how do we train Systems to understand the physical world through self-supervised learning. So what has worked for text is that you... Basic idea of self-supervised learning. You take a piece of data. You corrupt it in some ways.

For example, you mask a piece of it and you train a big neural net to predict what's missing in the input. And, you know, we could apply this to video, obviously, right? Take a video, show only the first part of the video to a system,

 and ask it to predict the reminder of the video. What is going to happen next in the future? And if a system is capable of doing this in any kind of reasonable fashion,

 it probably means it has understood something about the nature of the world. And I've tried to do this for a long time, but it didn't work until fairly recently. And so here is the basic generative architecture to do this, right?

Show the beginning of a video, mask the future of that video,  run the beginning of that video through an encoder,  and then maybe a predictor. Perhaps that predictor is conditioned on an action that you know is taking place in the video,

 and then ask the system to predict or reconstruct the missing part of the video. And the problem is that this never worked. It never worked because, or it never worked very well.

And the reason it never worked is that it's very difficult to predict all the details at the pixel level of what happens in the video. If I take a video of this room and I start from this side and I pan and I stop here and I ask the system,

 can you predict the reminder of the video? Of course the system is going to predict at some abstract level That we're in a conference room,  and there are people sitting, and, you know, there's probably a wall at some point,

 and the carpet is continuing. There is absolutely no way it can predict exactly what every one of you looks like. It's an impossible task. It cannot predict the texture on the wall or on the carpet in detail, right?

So the task of trying to predict at a pixel level is impossible. So... And in fact, when you do the experiment, this is some old papers, some of them from almost 10 years ago. When you train a system to do this kind of prediction,

 it makes very blurry prediction because it can't really predict what's going to happen,  so it predicts kind of an average of all the possible outcomes.

And there are ways to fix this in various ways using things like latent variable models and diffusion models,  but it's not producing the result that we'd like.

So my solution to this is what I call joint embedding predictive architecture, or JEPA. And there's a whole community building around this idea of JPEG, although it's fairly recent. It's only three years old, roughly.

But the idea is that instead of predicting at a pixel level,  you train a system to learn an abstract representation of the input so that predictions can take place in this abstract representation space,  not at the pixel level.

So, the two architectures are like this. On the left, you have generative architectures that all generative AI is based on. pixels or text or whatever. And on the right, this JEPA idea where both the observation,

 the X and the variable to predict Y are run through encoders and the prediction takes place in representation space. And to some extent, this is really at the root of intelligence,

 trying to find good representations of the world that we observe so that we can make prediction is at the root of intelligence,  but also science. We do this in science all the time.

I could explain everything in principle, everything that takes place in this room. Going back to very fundamental physics, quantum field theory, right? If I could measure the wave function of this room,

 perhaps I could make some level of prediction about what is going to happen in the next few seconds,  minutes, or hours by running quantum simulation. Of course, it's completely impractical. I cannot measure the wave function of this room,

 and there is no computer that's powerful enough to actually do this computation. So what do we do? We find abstractions that allow us to make those predictions.

In physics, for example, or in science in general, we have a hierarchy of such abstractions. Particles, atoms, molecules. In the inanimate world, materials, objects, machines. In the animate world, proteins, organelles, cells.

Organisms, individuals, societies, ecosystems, right? And every level in this hierarchy is a different field of science.

The field of science is almost defined by the level of abstraction at which we describe the part of the world that we want to understand.

So this process of finding a representation so we can make prediction is really at the basis of science. For example, in thermodynamics, If we want to understand the behavior of, let's say,

 a box filled with some gas, we cannot make predictions in terms of the position and velocities of each particle. But what we can do is, at the global level, predict that when we compress the gas,

 it's going to heat up, or when we heat it up, the pressure is going to go up. We have a law called PV equals nRT, the perfect gas law.

All of science is based on this idea that we need to find abstract representations that eliminate a lot of details about the observation that allows us to make predictions. So that's what JEPA really is based on.

So this idea has sort of driven me to come up with some sort of AI architecture that might be based on this idea,  which I call Object-Driven AI Systems.

And I describe in a paper that I put online in 2022, a path towards autonomous machine intelligence. And it's based on this idea of world model that I mentioned briefly earlier. What is a world model?

A world model is a model that, given an idea of the state of the world at time t,  and given a proposal for an action that an agent is imagining taking,  can the agent predict what the state of the world is going to be at time t plus 1?

 And again, it's not going to be the full state of the world. It's going to be some abstract representation of the state of the world that contains the relevant details. So if we have such a world model, Planning, right?

So we observe the current state of the world that goes through a perception system that estimates the current state of the world,  or at least the state of the world that we can currently observe.

We combine this with the content of a memory that contains the idea of the state of the world that we don't currently observe,  that maybe we remember. And then we feed that to our one model and we feed also a.

A candidate, a proposal for an action to take. And then we use a world model to predict the outcome of executing this sequence of actions.

We can feed that to an objective, an objective function that measures to what extent a particular task has been accomplished or not.

And then through search, we can search for a sequence of actions that actually satisfies this task objective. It's an optimization procedure of the type I was talking about earlier,

 and it's really akin to the type of planning and reasoning that humans and many animals are capable of doing. Now, in addition to the task objectives, we have to include guardrails,

 so things that will prevent the system from hurting people or doing stupid things or go beyond its design. And this is where safety comes in. We're going to design safe systems by building them perhaps along this blueprint,

 but also designing guardrails that will make sure the system behaves properly. So some of us here in the room are interested in the application of AI to science,  and this is really also a process we can use.

There, the action in a scientific experiment is really an intervention. Can I predict what the state of the physical system is going to be by doing an intervention? This is really how you test a theory,

 and the test is to check whether the hypothesis is satisfied at the other end. Now, if we have a model of the world, we might want to apply it autoregressively,

 recursively, because if we have a model that can predict the result of executing an action,  For a sequence, we can apply it repeatedly so that it can predict the outcome of a sequence. Now, the world, of course, is not deterministic,

 so this world model will probably have to get some latent variables which will parametrize the set of potential outcomes that may exist even in representation space.

And eventually, we'll have to make those models hierarchical so we can do hierarchical planning. If I want to plan a trip, let's say I'm flying back to New York Thursday.

If I have to plan my trip, I can't plan my trip from Seoul to New York. In terms of millisecond-by-millisecond muscle control, which is really the low-level control variables in a human body,  I have to plan at a higher level.

I need to get a taxi to the airport and catch a plane. Get a taxi to the airport. When in Seoul, I probably have to call one or ask the hotel to do that. If I were in New York in my office,  I could just go down on the street and have a taxi.

And then we can sort of hierarchically work our way down until we get to a point where we can execute the actions. That's completely unsolved. There's a lot of work to do here in hierarchical planning.

And the question is, how are we going to train those world models? I don't want to go into technical details,  but there is only two ways I know about to train such systems.

And the key concept here is that we have to prevent them from collapsing. So collapse is the problem that if we train a system to find a representation and make prediction in that representation space,

 the easiest representation space in which to make predictions is a representation space that does not contain any information about the input. It basically just produces a constant. Representation, and that's not a good model.

So you have to basically prevent the model from collapsing. These two classes of method, contrastive and non-contrastive,  I've become a big fan of non-contrastive method,  and I'll give you a few examples of those things working.

There used to be contrastive methods that we used for self-supervised learning for images. That's going back to the early 90s, a paper of mine from 1993 about Siamese networks and some more recent work on this.

But that turned out to be somewhat disappointing. What has turned out to work better is methods based on what's called destination,  where you really have two versions of the same encoder. So you take an image, you feed it to an encoder,

 and then you corrupt or transform this image,  or maybe it could be a video that you mask,  and you feed it to essentially the same encoder.

And you train the entire system through a predictor to predict the representation of the original image from the representation of the corrupted or transformed image. That's a JEPA architecture.

And one technique that works well is that you don't actually train the encoder on the right. You. You have it just use the weight of the one on the left,  and you use that encoder as a teacher for the other one.

And for some mysterious reason, it actually works. We don't completely understand why, theoretically. There's a few papers on this. But it does work, surprisingly well, with some caveat.

And in fact, it works so well that some of my colleagues at Meta have shown recently that this kind of method,

 a particular brand of method called Dyno, actually works surprisingly well and beats all the approaches that are based on supervised or weakly supervised learning.

The latest version is called Dyno V3. It's open source and you can just download it here. It's basically a generic image representation extraction that can be applied to a very wide range of vision applications and it works really,

 really well for all kinds of tasks and people are using them for all kinds of things from astronomy to medical imaging to everything else. Now, with this type of method, Dyno, we've been able to train a world model to do some planning.

So, essentially, on top of the Dyno representation, we train a predictor that is conditioned on actions. It's trained from synthetic video of robots. And then we can use it for planning. So here's a short video that shows an example of this.

The task here is to use a robot to push all of those little blue chips into a configuration that looks like the one on the top. So the top row is kind of a target state.

And what you see at the bottom is a sequence of actions planned by the world model to,  on top of the dino representation, to Push all those blue chips in their proper position. It's a very complex problem.

The underlying dynamics behind this is really complex,  and we've applied this to a number of different situations,  which seems to work pretty well, at least at some level of complexity. We're still working on this.

Another one of our colleague, Amir Bar, has worked on a kind of a similar idea except with a different encoder for navigation. He's a scientist at FAIR as well.

And there, the purpose is to get a system to understand how the view of the world changes when you move. Okay, so this is trained with Robot video. And you can basically tell it, OK, go to a place where you're in front of the door,

 of the red door. You can see a red door. And the system can sort of plan a path towards the goal you give it,  as long as this goal is visible in its view. It's been applied to a bunch of other things. This was presented at the last CVPR.

Actually, you won some sort of best paper award. So this works well, but the question is how do we train this JPEG architecture end-to-end? Basically by training the encoder together with the predictor.

There's some work that goes back a couple of years called iJPEG where we train certain architecture together with the encoder. To produce good representations of images, and we've gotten extremely good results on image representation.

And more recently, we applied this to video. So this is called VideoJappa. There was a first paper last year about this and a more recent paper together with Open Source Code and Open Source Model more recently.

And it's a system that basically learns to fill in the blanks in a video. And it works very well for classifying various videos,  but what's more surprising is that it's acquired some level of common sense.

So if you show it a video where something impossible, unphysical occurs,  like an object disappears or changes shape or something like that spontaneously,  The prediction error of that system, the internal prediction error, shoots up.

And so it tells you there is something impossible going on here. It's the first time I've seen a model completely self-supervised that tells you something impossible is happening in video.

That's really how we actually try to figure out whether little children have acquired a concept. You see how surprised they are by a scenario that we show them. So that's a very interesting result, very promising.

The VJPAD2 model was recently released and there's a version of this that is trained action conditions so it can perform planning for for robot and execute a number of tasks. I hope I have enough time to show you this video.

So here's an example of this video JEPA model that was trained with sort of generic videos and then fine-tuned with action condition model,  but not with this particular robot.

And then it searches through a sequence of actions so that the world will look like the goal. At the representation level and plans the sequence of actions to, for example, pick up a cup and move it.

Of course, you can do those tasks by programming the system or training by imitation. But this is, you know, essentially zero shot. You're not training the system for any of those tasks.

It figures out how to solve that because of its mental model. This is really what we're after. So we have another set of techniques that we're working on which are extremely promising to train those JPAR architectures using,

 it's kind of a measure of information. I'm not going to explain to you how it works, but this is, in my opinion,  the most promising aspect of it.

So the challenge that we have is both theoretical Maximizing an information measure and we don't know how to do this because we only have upper bounds and not lower bounds on information measure. But we're still doing this.

But let me conclude. I have essentially a number of recommendations for making significant progress in AI. Of course, if you want to work in industry,  you have to work on LLMs because that's where applications are in the short term.

But in the long term, we should abandon generative models in favor of those JEPA architectures, which are non-generative.

I've been in probabilistic modeling because in this framework, probabilistic modeling really doesn't make any sense anymore. There is another paradigm that I've been advertising for a number of years called energy-based models.

I've been on Kochatsy method in favor of those regularized methods, information maximization, which I only mentioned,  didn't have time to explain.

And of course, I've been saying for over 10 years, Minimize the use of reinforcement learning because it's extremely inefficient. And I'm far from being the only person with this opinion anymore.

So if you're interested in pushing AI towards human-level AI, you shouldn't really work on LLMs. You should work on the stuff that LLMs cannot do at the moment, which is this list of capabilities.

My prediction is that LLMs will become obsolete within the next several years, perhaps within five years or so. Jetpack-style architectures will become prevalent, probably with influence to optimization. Real-time vision.

I know there's a lot of people who are interested in hardware in Korea,  so real-time vision will use a combination of things like convolutional architectures and transformers and various other things.

But more is coming, so the search for better architectures is not over. And then for hardware memory limitation, memory capacity has become a limitation.

And I always had an interest in sort of exotic future technology like analog computation and things like that,

 but I've become extremely skeptical about it because we can't use hardware multiplexing as well as we can do with digital hardware. So with that, thank you very much.

Speaker 1:
Thank you very much, Professor LeCun, for your insights. So he has shared his views by giving predictions and recommendations. You can now return to your seats once you are done. Okay, thank you very much.

And he introduced various technologies and systems for training models. Once again, let's give him a big round of applause for his insight. Moving forward, we will now have the second keynote speech.

And for that, Professor Yejin Choi from Stanford University will be delivering her presentation. And this presentation is titled, Democratizing Generative AI by Transcending, Scaling.

Speaker 2:
Laws.

Speaker 1:
And she is one of the world's leading AI researchers known for her pioneering work in common sense reasoning and ethical AI. Let's give her a big hand to welcome her to the stage.

Speaker 4:
Right, so I'm very excited to tell you about Mission Impossible. Democratizing Generative AI by Transcending Scaling Laws. So whenever I talk about this, people just roll their eyes that my dream is just impossible.

And I want to say something really against that in this talk. So a case in point, this is the prevalent narrative that's repeated to us that,

 you know, when you ask, for example, how can Indian startups create the foundation models for India,  Sam might tell you, don't bother. I love OpenAI, by the way. I love ChatGPT. I don't have anything against OpenAI.

However, I think it's really important that all of us still work on making AI ourselves. So that's going to be the theme of my talk today. So when you feel that there's no hope, let's look at the history, because time and again in history,

 there's a David and Goliath-like situation. I mean, in some sense, the currently really successful tech companies that make frontier LLMs didn't exist until they existed.

Therefore, we can always think about What clever things you could do in order to grow new power. So in the era of extreme scale neural models, however, we have to have Some advantage. You cannot have no advantage.

You cannot have no GPUs, no nothing, and then somehow hope for luck because it's not going to come to you. So I think there are three really important components to innovate. One is unconventional data.

The second is unconventional algorithms. This is a big portion of my talk,  but later I'm going to visit unconventional collaborations because these are all really very important.

So the era of a brute scaling scale brute force scaling is over. As also alluded by Yann LeCun in his talk. And when Ilya Suskivor said that pre-training as we know it will end,  in his Test Time Achievement Award talk at NeurIPS last year,

 You know, it seems like a bit... Exaggerated, perhaps, but it does definitely give us this feel of, oh, there's this curveball coming at us that,  you know, this narrative about just the bigger the better is sort of like being challenged.

And then this year, that sort of concern seems to gain more traction in that,  you know, some people are now wondering whether, hmm,  what if AI doesn't get much better than what we have right now? I mean, it might, I think it will go better,

 but these are valid concerns because we cannot just make things bigger. We need to have a different game plan. I mean, Elias Kiefer said that there's a concern because Compute increases but data, internet data doesn't increase as much.

It's our fossil fuel that's running out. And so there are three broad ways to cope with this data saturation situation. One is to learn better and faster with limited data. This is basically what humans are able to do.

So we need to really figure out how to do that. But more immediate strategy could really be synthesizing new data. And by the way, If you just synthesize data that's similar to the internet data,

 that's not an advantage because all the other companies already downloaded the entirety of the internet. Therefore, it's really important to be able to synthesize data that's qualitatively so different from whatever is on the internet.

And then finally, I think we really need to rethink about the ML, DL,

 deep learning techniques in order to challenge all the conventions because I think we can really innovate a lot in order to continue scaling intelligence without necessarily continuing scaling GPUs. Okay, so...

In this talk, I will show you three case studies. That corroborates the position that I'm trying to make in this talk. And let's begin with Pro RL, which is prolonged version of RL.

So, before I go there, let me do a brief history of this year,  which is the rise of large reasoning models as opposed to large language models. So, you know, DeepSec R1 dropped earlier this year, and then, you know, other models dropped,

 and here's humanity's last exam benchmark result. I'm pretty sure that this is not our last exam. There will be a lot more exams. But yeah, models are doing really better and better faster. And there's one commonality.

Especially among the top performers,  which is that There's always this long thought or chain of thought that is based on the power of reinforcement learning and there's this major shift from imitation learning to exploration learning.

This all together makes it look like reinforcement learning is maybe the way to go. You know, Yann LeCun said, abandon that. But, you know, like the vibe in the media has been very much that,  wow, this is like pulling some kind of magic.

However, and these are some of the papers that support that vibe that, you know,  this one paper says, SFT memorizes, ADEL generalizes,  therefore ADEL is better than whatever SFT is.

Let me give you a little bit of overview about what SFT means, by the way,  because the rest of my talk kind of relies on these basic terminologies or concept of how LLMs are made.

So it always starts from pre-training on the internet data, and then there's this SFT,  or sequential fine-tuning, which is supervised training on lots of curated exam problems.

So it's almost similar to how when we used to be students and then,  you know, prepare for final exam, suddenly we start solving a lot of exam problems for practice. So that's what happens during SFT.

And then there's ADEL, which is Reinforcement Learning,  but that also happens on curated exam-style problems for most of the time today. But anyway, SFT and RL together usually is known as fine-tuning,

 whereas pre-training and SFT together is imitation learning,  and then RL is exploration learning. Imitation learning means, you know, you just imitate whatever is given to you and then try to copy that behavior without questioning,

 whereas exploration lets the model to explore for itself. Okay, so with that in mind, okay, these papers say SFT memorizes, RL generalizes,  and this other paper also shares a similar narrative.

But there are Mind you, other papers that says otherwise. So, you know, these papers raise interesting question. Does reinforcement learning really incentivize reasoning capability in LLMs beyond the base model?

So that paper I realized that the base model already had a lot of good answers in the top probability zone. Ada was just reshuffling the probability space a little bit so that the correct answers rise to the top.

But the correct answers were already in the base model defined. So these papers sort of pour cold water. on the RL party. To make things even more curious, a recent paper from UW says, superior rewards, rethinking training signals in RL VR.

VR is verifiable reward. So curiously, what they did was, instead of giving correct rewards to your LLM,  they give them Wrong rewards, random rewards, and whatever you do, performance goes up. How can this be possible?

But they reported that it's very much the case, especially for Kuang from China. Whatever you do, performance goes up because the base model knows so much. What's strange, even more strange, is that this doesn't work for every single LLM.

So with Lama, not as much. With Olmo, almost nothing. So some of these findings are very, very confusing right now. And then the insight I want to offer to you today is that there's this chemistry between base LLMs and then RL,

 And the other insight that I want to share with you is that conclusions drawn from effortless RL is not the same as conclusions drawn from effortful RL. So let me show you how to put efforts into RL. So that's our new paper where, you know,

 basically the spirit of this message is that Rome wasn't built in a day and we have to put efforts. And what's to my mind quite surprising about my own paper,  but I was surprised that we can go that far with such a small model,

 1.5 billion parameter model, to go quite far. I thought this model is too small to do really hard math problems,  but we were able to do strikingly well. There are some jargons. Don't worry about this.

I just tell you that you may have heard about GRPO because it's from DeepSeek R1.  If not, don't worry about it. There's something called DAPO from ByteDance. Don't worry about it either.

There are some known recipes already reported in the literature. The only thing we really do is to execute it with care. That's what we do. We change just a little bit some of the details.

If you're interested, you can check out the detail of the paper. But let me give you a very, very brief flavor of what exactly we are trying to do with care,  okay?

So what you see on the left-hand side is During training, you want to see this loss going down. It's entropy that should go down during training. The longer you train without L, the left-handed side should go down.

The right-handed side is about Accuracy on your test set. That should go up. So the higher is the better. And then ideally, what we try to do during training is that training loss goes down,

 and then accidentally that somehow correlates really well with the test performance going up. Okay? Now, if you really stare at these colors,

 The green bar looks the most promising during training because it's going down the most aggressively. But when you look at test performance, it was looking really good at the beginning but it starts plateauing, right?

So that's not what we actually want. What you really want It's not this orange and pink bars either because Training goes up too much and then test is also like not going well either.

So what you really want is this Goldilocks zone where the entropy is neither too low nor too high. What is entropy by the way? So entropy is about the amount of creativity that these base models will retain.

You kind of want We need to have a little bit of creativity left in the model as opposed to wiping out completely. This is something to do with exploitation and exploration trade-off,

 which is one of these key concepts of RL where you want to be able to exploit what you have learned,  but in the end you want to remain open-minded because you should be able to explore.

It's a little bit similar to how humans As we get older, we learn more. We always have this risk of being overly confident about ourselves and unable to learn new concepts and new knowledge and new ideas and new culture.

So we don't want that. We want to be able to be open-minded for prolonged time and it turns out that's what's important for reinforcement learning as well.

There are many more details like this going on and on about how to keep your training model to maintain the right level of entropy on and on. Don't worry about the details.

But the bottom line is that when you do it that way and then go with one very long hero run, hero run meaning like We do this only once. We don't have enough compute to do it twice.

You just somehow make the model keep going at the right entropy level. And so in doing so, you may need to change some hyperparameters in the middle. But anyway, to our big surprise, we were doing better than distilled R1,

 the official one of this R1 DIP6 R1 model of the same size after prolonged RL. So that was exciting, but even better, okay, now this was unexpected, but even better,

 we were able to do better than 7b model of DeepSeq R1. That's really hard to beat,  especially if you use 1.5 billion parameter model in this hard math problems.

It's really hard to achieve, but sometimes we were doing even better than DeepSeq R1, 7b. Okay, so going back to this earlier point,  I want to reemphasize that you got to put efforts into what you do.

You cannot just declare that it doesn't work because it's too small. That's not true. We can always make things work even at the smaller size. It's a little bit like whiskey making or maybe since I'm in Korea You know, like really good.

I don't know makgeolli or kimchi all of these. ...require very careful control of temperature, pressure, whatever. So similarly, when you bake at all for a prolonged time,  you gotta give some care and then control for the environment.

But having said all of this, I will... Remind you that this effort doesn't work on top of any model. For example, if you started with GPT-2, I really don't think anything good will happen. GPT-2 is too weak of a model.

I don't think it's actually possible to bring much out of that model just by sheer RL,  even if it's effortful. So there's something about the chemistry between the base model And when ADEL actually takes off.

So I will get back to that point later again,  but something else I want to add on is that conclusions from effortful SFT is not the same as effortless SFT. So there's a bit of a parallel that I want to make,

 and that has a lot to do with why Quen was so good with RL,  to the point that you can just give any random rewards and the performances still go up. Why?

So that's the chemistry, because that base model has acquired a lot of reasoning knowledge by measures that's not public. But one can speculate. Okay, so on the speculation note, I want to next present prismatic synthesis.

So this is Especially meaningful for me to present because the first author Jaehoon is a Korean student. Really doing innovative work with me. And so, revisiting this earlier message that Ilya said that you know pre training.

As we know, it will end because compute is growing, data not growing. Since data is not growing enough, a lot of people, companies, do do synthetic data generation,

 but usually the common practice is to use the largest, strongest teacher model. I don't like it. Why should we always rely on the largest model? Maybe we should think about how to make better use of a smaller model.

So that's what I want to tell you in this segment. There's obvious concerns about synthetic data in general that it could lead to mode collapse. So papers like this says...

The curse of recurrence, training on generated data, makes models forget and deteriorate. It's almost like a medical disease. You cannot let AI to consume its own self-generated data for too long.

But that's all true only if you do things without effort. If you are doing lazy version of it, of course,  you're going to encounter that kind of problem,  but we can totally avoid that problem.

I think this is really, really promising direction. So before I tell you what we did, let me start with the end result,  which is that so on the x-axis, it's data size,  so the larger synthetic data you have on the y-axis,

 the performance goes up. Usually when you do the lazy version, that's the pink chart. So the performance goes up but it starts plateauing. On the other hand, the prismatic synthesis, our method, the performance...

Charges ahead pretty well to the point where we were able to experiment. So let me tell you super super condensed version of what we did. So we started with As a teacher model, DeepSeek R1 32B parameter model.

We're going to do really difficult math and then try to be state-of-the-art here. 32B is 20 times smaller than the largest teacher model that usually people use that largest one,

 671B parameter model, but we are using 20 times smaller model as teacher model. We overgenerate. We overgenerate so much and then we are going to filter that data really aggressively.

We just throw out the vast majority of the data that we just generated by looking at the diversity of the data that we just generated. There's a bit of technical details here that I'm not going to go into,

 but the intuition is that we project each data point in the gradient-based vector space,  and then we do tensorized k-means clustering to find which regions are over-represented and under-represented.

We throw out all the over-represented data points and then maintain the less represented ones. There are other quality filters, I will not go into the detail,  but we rotate this around until we collect about a million data points.

I am so excited about the ending result that when we compare, so ours is Prism Math,  which is trained on prismatic synthesis data, compared to R1D Steel 7B OpenThinker 2,

 these are the state-of-the-art of that time, at the time of this paper writing,  and we were able to do better than all the others. Importantly, OpenThinker were R1 distilled.

They were both distilling from The strongest teacher model which is DeepSeek R1 671 billion parameter model,  whereas ours distilled from 20 times a smaller model.

It's the same amount of data we distilled compared to OpenThinker, but we still do better. And as a matter of fact, OpenThinker, only this We only synthesize the solution, not the problem.

So the problems are human-written problems on the internet, whereas ours is fully synthetic. We just wanted to see how crazy we can go with the degree of synthetic data generation,  and we find that we can go push this all the way.

So the concluding thought so far is that reasoning really requires the data that transcends the internet data because hard math problems Don't really exist in sufficient quantity in the existing internet data.

So that's one example where synthetic data could really go very far. And there's so many other examples in the business use cases where you really want a particular application scenario supported,

 but internet data somehow is not good enough. Synthetic data with really good care and effort could go very far. In fact, I would I argue that RL, reinforcement learning,

 is only a different form of a synthetic data generation because your exploration is basically an act of synthesizing data. RL or not, existing methods often lack the bird-eye view on the overall diversity,

 so care is important for that, and diversification really, really helps, especially for out-of-domain generalization. Okay,

 let me now do the last part to reinforce the pre-training which is super new work that's very much I wanted to work on this for a while.

I didn't think this would actually work, but the lesson learned from this is that I shouldn't draw any conclusion before trying anything at all. Okay, so what we do is reinforcement as a pre-training objective.

This is such a weird thing to do. Usually reinforcement learning is the last phase of fine-tuning act. So we're going to bring this earlier to the pre-training. So pre-training usually goes to something like this.

You have an input, which is some sequence of words,  and then you have this model to predict which one word or one token comes next. So that's Next Token Prediction, or NTP. That's what happens during pre-training.

You go on and on predicting which one token comes next. 이 모델은 다음 토큰이 들어올지 예측하기 전에 스스로를 생각하는 것입니다. This is the reward in this case is information gain of being able to better predict next tokens with or without thought.

So a lot of the times you do not need a thought in order to predict the next token because it's too obvious that there's no reward that's meaningful. But if the next token requires internal thought,

 then we really encourage the model to think for itself before saying the next token. It's a little bit getting more relatively similar to how humans would learn,  which is that usually we really want to think for ourselves,

 I think, Other than just keep memorizing what word comes next. So that's what we do. Um, being very, very brief about the technical details, uh,  it's a conditional probability of next token given previous tokens and,

 uh, with or without the thought that becomes the reward. Um, I will Let me think. I will skip over these details and then just highlight the fundamental empirical results. So the questions we wanted to check is whether our method, RLP,

 can improve the reasoning ability of the base model even without any Task specific fine-tuning applied to the base model. This is usually what happens. In all the other base model, quote unquote, like QN base model.

They all went through special fine-tuning for being better ready for hard math reasoning,  but ours actually operate before that happens.

And then whether that gain can survive even after applying the regular sequence of SFT and then followed by RLVR. This is a really challenging empirical setting. I'm probably showing a bit too much details, so I'll be very brief about this,

 but we do this both the token-matched and flop-matched evaluation setting. And the conclusion is that when we do this on very small model, when 3.2... 1.7 billion parameter model. Uh, quite well, even in that game, sir.

Survives even after applying SFTR, not LVR, which is very surprising result. So in some sense, it's like it's better to introduce reasoning capability earlier in the pipeline of pre-training,

 you know, like Up until now, a lot of this pre-training recipe,  post-training recipe has been almost like fixed and,  you know, you don't dare to change it.

But it might be that we just have to try and see what happens instead of concluding because large other companies have done a particular way.

That's why we should copy and paste and do the same thing versus trying different ideas in case it actually works even better. Okay, so yeah, probably a bit too much detail.

So let me skip over some of this and then move to my Concluding remark,  but yeah, maybe I'll just say that this works really well even for 12B parameter model. We were able to test this and this is a flop match as opposed to token match,

 meaning compute is matched instead of a token amount. Our RLP has seen less pre-training tokens and yet it performs even better than the counterpart. Okay, so concluding remark. I have a lot to say about this earlier point because, you know,

 when I just walk around and tell people that let's make small models better,  I really get a lot of headwind, you know,  the naysayers who really want to say that just to call ChatGPT.

I want to say, no, let's try doing something else on top because, I mean, please do call ChatGPT. I'm not against it, but let's also make our own things that are small.

The reason why I think it's so important is because AI is so powerful, we have to democratize it. We cannot just leave it at the hands of a few powerful entities. And so, fundamentally, AI should be of humans, by humans, and for humans.

In terms of the ownership, AI should belong to and originate from humans,  reflecting the values and the characteristics of the human society at large.

AI should be by humans in terms of creation in that AI is developed by and governed by people worldwide as opposed to a small group of corporations or countries. I really want this to be at the hands of everyone. And then AI for humans.

AI should really serve humans and importantly all humans, not just some people in power. Otherwise, if we don't do anything about it, I think we could really be...

Facing a future in which AI might be serving just AI, or even worse, humans will be serving AI. I think we really got to do something here all together. So, going back to these earlier points that I said we need unconventional data because,

 you know, current generative AI is all about data. Yann LeCun is so right. LLMs are doomed because the dependence on the data is ridiculous compared to human intelligence.

But for the time being, it's a very Compelling technology that can enable a lot of business use cases. Therefore, if we want to build small LLMs that are capable, data is the key.

And then algorithms can be super empowering, especially for small models, to close the gap against the larger counterpart. Now, unconventional collaboration.

I want to really drill down to this because a lot of people tend to think not outside the box. You just follow whatever was the previous way is the only way. So we really need to think big.

And this open thought team was really shocking to me. I'm a small part of the team, but I am just, you know, insignificant middle author.

The problem with academic research, I realize, is that everybody wants to be the first author and last author. Therefore, everybody is chopping papers into the smallest pieces imaginable.

And then they all emphasize how many papers they published at NeurIPS. It's not as meaningful as we might think.

I think this collaboration is really very impressive because it was like crossing across many universities and different companies and even including startups. And I was so proud to be just part of an insignificant member of the team.

What they did is they did use the largest Strongest. Teacher models like DeepSeek R1, the largest model, and later QN3. In order to make one million data points,

 this required just like unbelievable amount of coordination and hard labors and quality check and everything. But what really blew my mind away later is that it just works so well.

When you have a high quality data curated that way, and then, you know, we just open data. It's amazing fit. So well so that it actually works better than some of the models that went through RL. This is a beautiful result in my mind.

So, yeah. There are a very long list of co-authors across so many different universities and institutes. So that's one example of unusual collaboration. I think, you know, at the end of the day Nothing is easy in life. No pain, no gain.

Everything is doable with effort, so don't be afraid. Collaboration is powerful, but you gotta control with your ego thing, because that's really the enemy. That's always been my motto in my life, and it's super important.

Let me share a few words about HAI, where I spend my time with,  because they envision that they want to really create these HAI labs across the world. So they want to have this new, very unconventional way of doing things together.

They're interested in not only open foundation models, but physical AI, embodied AI, ambient intelligence, etc. So we're already working on building some labs around the globe like A lot of progress is made with Switzerland and Singapore.

Because I'm from Korea, I really wish that we can do something with Korea as well. So at least HAI is seriously interested in. Thank you.

Speaker 1:
Thank you, Professor, for your very inspiring presentation. So Professor talked about democratizing generative AI by transcending scaling laws. She also introduced some of the reasoning methods such as RLP.

Once again, I would like to thank for your inspiring And at the end of her presentation,  she mentioned how AI is important for our human society. And I'm sure that everyone in this room will agree with her.

And that's why we are here today to explore the future of AI and its impact for our society. Moving forward, we will now have the roundtable discussion.

Leading scholars and experts from around the world will share their perspective on the advancement of artificial intelligence,  global cooperation, and the ethical and responsible development of AI technologies.

The roundtable will be chaired by Mr. Kyung Hoon Bae, Deputy Prime Minister and Minister of Science and ICT,  who has provided visionary leadership and policy direction in shaping Korea's national AI strategy and research ecosystem.

Under his guidance, we look forward to a thought-provoking and insightful discussion on how AI can continue to evolve responsibly and inclusively.

Today's roundtable brings together some of the world's most distinguished AI scholars and leaders who have been shaping the future of artificial intelligence through groundbreaking research and innovation.

And I hope the insights shared in this discussion will offer valuable guidance for the future of AI policy research and international collaboration.

And for the round table, please welcome Minister Kyung-Eun Bae to the stage, along with our distinguished panelists,  Professor Yann LeCun, co-director of the Global AI Frontier Lab, Professor Yejin Cho from Stanford University,

 and Professor Kyung-Yeon Cho, co-director of the Global AI Frontier Lab, and Professor Ki-Yoon Kim,  director of the National AI Research Lab. Let's give them a big, big round of applause. And Minister, the floor is yours.

Speaker 2:
Today, as a moderator, I prepared three topics and I'm very pleased to have this opportunity to host today's roundtable discussion with AI scholars,  where we will discuss the future development of AI and key issues So in this session,

 I will ask some questions based on each topic and invite the distinguished scholars to share their diverse perspectives. Because we have not enough time, after I explain about topics,  I will give a question step by step on each topic.

Thank you. First, let me begin by introducing the attendees. Although the most guests already know, Professor Yann LeCun is a professor at New York University,  chief AI scientist and matter, and one of the top four gurus in AI.

I'm leading binary research in deep learning. Professor Yejin Choi is a professor at Stanford University and Distinguished Science of Language and Cognition Research and Media.

Recognized as one of the 100 most influential people in AI by Time Magazine and is actively conducting research in Human-centered AI. And next, the Professor Kyunghyun Cho is a professor at New York University,

 a non-profit devising neural network-based machine translation which revolutionizes translation models. And the final, Professor Kim Ki-Eung is a professor at KAIST, leading research in machine learning,

 reinforcement learning, modeling, and algorithms. And let's now move on to the first topic. The first topic is the rapid expansion of AI has become a new driving force in both The economy and everyday life.

However, it also raises new social issues such as bias, privacy violations, and deep fakes. How can we build a trustworthy AI ecosystem without Sniper ring technological innovation.

Professor Jan Nukun How do you think we should address this issue?

Speaker 5:
Thank you. Well, I think Yejin mentioned that, Yoshua Bengio also. I disagree with Yoshua on a number of topics,  but there is one that we agree on,  which is that AI should become sort of a common resource.

And I've been advocating for a number of years the fact that AI methods are becoming kind of an infrastructure of The economy of technology. And there is always a big incentive for infrastructure software to become open source.

I think it's true of AI as well. What is slightly complicated at the moment is that the best open source models are all from China,  and most of the American companies have been climbing up and becoming more secretive,

 which I think is ultimately self-destructive. It might actually be good in the short term for those companies because they want to preserve their secrets,  but I think in the long run, this is not how it's going to play out.

We're going to see a repeat of the history of the infrastructure of the Internet that occurred in the 1990s,  where most of the software infrastructure of the Internet, above TCP IP and the basic protocols,  was actually closed.

Proprietary hardware, proprietary operating systems, and web servers. But eventually, all of that got completely wiped out and now the entire infrastructure of the Internet is open source. I think the same is going to happen for AI.

And in fact, I think the only way forward is for AI to be AI platforms,  at least foundation models, to be open source. The reason being that In the future in which we get all of our information through the intermediary of an AI assistant,

 then How is it going to work if all of those AI systems are controlled by a handful of companies on the west coast of the US or in China?

No country in the world wants The information getting to all the cities and being controlled by a small number of private companies or another government,  right?

So there will be a push for AI sovereignty and by necessity open platforms for AI.

Speaker 2:
Okay, thank you. He emphasized about the open source ecosystem. And next, Professor Kim Ki-heung, what specific measures can we take to ensure AI technologies are developed with the safety and in a truthworthy manner?

Speaker 3:
Sorry. I didn't know this roundtable will attract so much media attention,  so I thought I should be a little bit careful what I say. So I have this cheat sheet so let me let me let me do some cheating so Let me see.

I believe most researchers in academia and also many companies, I think my opinion will resonate with Yann LeCun. Many researchers in academia and many companies are exporting ways to foster global AI ecosystem by embracing open practices,

 like from sharing codes and data sets to publishing reproducible results. In line with these practices,

 I believe open sourcing is one of the important principles for improving trust in AI systems because it provides transparency and accountability.

So in my view, open practices are encouraged, they encourage also technological innovation at the same time. Especially in an inclusive way, if you look at most of the frontier model development,  it's mostly happening in the industry.

And in the academia,  we often worry about the gap between what our graduate students do for research at school and what they work on after graduation. I cannot imagine doing any, well, meaningful academic research. Maybe this is too much.

But any meaningful academic research on foundation models academia and the industry at the same time.

Speaker 2:
Okay, thank you. Next, Professor Choi Yejin, what are your thoughts on ensuring AI reliability and safety?

Speaker 4:
So there's a short-term thing we can do and then long-term thing we can do. In the short term, it's all about data because, you know, it's like LLMs are what they ate.

And internet data, unfortunately, It's not really the best representation of humanity. It has a lot of questionable content and LLMs just learn from the worst examples of ourselves.

Now, the thing about data, though, is that, as alluded in my talk, the quality and the scope really matters. And creating the right kind of data from a single institute, from a single country, is hard.

So this is where we could really collaborate and share the data. The challenge right now with open source, or let's just say open weight,  models is that they often don't share the data. For a variety of reasons.

And I think that's one of the limiting factors about AI safety for the time being. That's the short term. Maybe in the medium term, I think we need to invest more into fundamental research to really understand LLMs better.

But really in the long term,  I think it's really important that we really rethink about the current paradigm of LLM training because I think no matter what we do,  By patching things up through fine-tuning with more data points,

 it's just not going to be bullet-proof safe. It's always going to have some strange jagged intelligence such that it's going to have some jailbreaking weaknesses baked in the model.

So in order to really avoid that, I think we need to also seek for really entirely different alternative ways of training LLMs.

Speaker 2:
Thank you for your valuable insight. And next, Professor Cho Kyung-hyun, do you have any additional thoughts about this topic?

Speaker 3:
Yeah,  I think the one thing that we haven't actually talked about yet is the thinking about the full life cycle of the data as well as the AI algorithms. At the end of the day, when you look at this kind of life cycle,

 What we are missing in this discussion is the actual users of the AI as well as people who are going to be subjected to to the widespread use of this AI algorithms. So for instance, if you think about it from the perspective,

 it is not only the developers or the researchers of the AI who actually have say in this.

But it's probable that the people who are going to use them and who are going to be the part of this whole ecosystem will have to I have as much say as we have now.

So how are we going to create a Whole ecosystem regulatory framework as well as a political system that allows people to have a say and raise their voice in how AI systems are built and how their information is going to be used.

I think that's going to be one thing that we will have to think about a lot as everyone uses AI and then AI is going to be deployed in every corner of the society.

Speaker 2:
Thank you. As you've heard today, innovation and safety in AI are inseparable. We must continue our effort to build Trustworthy system while advancing technological innovation. And let's now move on to the second topic.

AI has become a core element of national security and industrial competitiveness and competition among countries. In this field, it is fierce.

However, considering the complexity of AI technologies, international cooperation is essential to address ethical issues and other challenges.

What are the obstacles to moving beyond the national AI competition to global cooperation and how can we overcome them? Thank you. If possible, Professor Yann LeCun, I'd like to ask for your thoughts on this issue.

Speaker 5:
Well,  I think there was some opinions formulated by some of my colleagues over the last decade that thought AI was qualitatively different from previous technology when it comes to geopolitics and defense.

And there's been a lot of debate about whether AI should be banned as a component to weapon systems,  for example. And that was seen universally as very naive because AI is already widely used in that context.

And then a lot of people changed their mind after the invasion of Ukraine by Russia. Because what is becoming quite clear is that the new style of conflict, of armed conflict,  is through AI systems, essentially, through drones.

And the drones are becoming increasingly autonomous. Because of course it's very difficult to control them by radio because of jamming and various things. So what Ukraine is doing, and a lot of countries in Europe are working on this,

 is systems that are at least semi-autonomous, can recognize targets and everything. And so I think the is evolving because of the geopolitical situation. That said, against the use of AI in weapon systems,

 now realize that it can actually be used to defend liberal democracy in Europe and perhaps in,  of course, other places. So that original opinion is now seen as naive.

Speaker 2:
Thank you. Next, Professor Choi Yejin, what do you see as the biggest obstacles to international AI cooperation?

Speaker 4:
In order to really enable international collaborations, we, I think, really need to think outside the box. The current situation is really unprecedented, and we cannot just go along with our usual way of thinking about doing things.

So there are maybe two challenges to deal with. One is that in order to do some reasonable amount of research, we do need resources, especially GPUs. And sometimes it could be a little bit hard to have enough of it,

 in which case we could try to collaborate across the different entities across different countries.

And the open thought that I showed you earlier was really a collaboration in which whoever could contribute either talent or resources or data were part of the team. Relatedly, another challenge is talent.

So the thing about generative AI is that it's both really easy, at the same time, it's really hard.

It's really easy in the sense that there are a lot of newcomers Who are just innovating to our surprise and then write really high quality papers or even,  you know,

 DeepSig R1 could be one example in which we didn't expect to see a model of that capability coming from China until it did happen. It could be really easy to catch up.

On the other hand, it feels really hard to catch up because it really requires knowing Really complex literature,  inside out, understanding and being able to differentiate noise from signals,

 and that requires really rapid adaptation and teaching each other,  learning together through teamwork, and that requires really rethinking about how we develop talent. And going back to Yann LeCun's earlier point about open source,

 One of the best ways to attract top talent is doing open source. If your company does open source, it's such a talent magnet.

Right now a lot of Chinese students in U.S. choose to leave U.S. to join Chinese LLM companies because it just looks so cool. And so that's some other consideration about how do we do better in this,

 you know, new confusing landscape is that we do open source in order to grow and attract top talent.

Speaker 2:
Okay, thank you. Professor Cho Kyung Hyun. How do you think we can foster international cooperation in the field of AI? And could you recommend to AI company in Korea?

Speaker 3:
Yeah, I believe any kind of collaboration, international or not. It can only work if there is a certain level of mutual understanding among the parties that are trying to work together.

And then that kind of mutual understanding, particularly in the context of the AI research,  comes from the fact that the researchers ...interact with each other, preferably working together on various research projects,

 and then thereby they learn to know how to interact with each other. And for that to happen, we need to really encourage this kind of, say, cross-border collaboration.

And then not really just the superficial ones, but the ones where we actually get to sit down together. However far they are from each other and then try to work on something that is really deep and for a prolonged duration.

So just to advertise the Global AI Frontier Lab. That's what we've been doing over the past, let's say, year or so. Trying to bring the top talents from Korea to NYU.

Let them stay there for anywhere between three months to nine months at a time. And I'm there by all the students from Korea and students at NYU. They get to know each other and then they start to speak the same language.

And then being able to speak the same language in terms of AI is how we actually foster the collaboration. And then. I believe that actually goes in the same way to the companies in Korea as well.

So you cannot simply say that we are going to have the top talents within their company, within the Korea. That just doesn't work like that. They need to be proactively reaching out, go out there,

 and interact with Not only the other companies,  but also with the universities and students. And then trying to know what is the language that is being spoken. Internationally when it comes to AI research.

And then trying to increase this level of the collaboration by increasing this level of the mutual understanding.

Speaker 2:
Thank you for your valuable insight. And next, Professor Kim Ki-Eung, what is your take on So same issue.

Speaker 3:
Professor Yan Li-kun, Yejin Choi, you aptly pointed out geopolitical tension as one of the obstacles for collaboration. I think it is, yes, I think it's the most important factor,

 but I would like to stress maybe my second thought about another factor is that in order to have a good team collaboration,  Each member, I think, has to have different strengths and weaknesses. So I am very strong in A, I'm weak in B,

 but I have a partner who is strong in B,  weak in A. Let's work together, have something good together. But in the field of AI, I think every nation is aiming for the same thing. The full stack of AI development.

From chip to software and talents. And because of this, I think, Since everybody is aiming for the same thing,  that's the most important obstacle that's hindering the collaboration. So it's very difficult to say how to address this.

Maybe one of the The thing that's behind this is maybe Profit, money is important. So I'd rather suggest when we do international collaboration,  rather than working on something that would be interesting to the company or industry,

 but rather we work on something useful for the humankind, such as climate change,  health care, and many something that would have some humanitarian benefit.

Speaker 2:
Okay, thank you. Today we've gained valuable insight into why international collaboration is so important in AI development,  and how we can approach it with a shared global vision. Next, let's now move on to the final topic.

I would like to take a moment to discuss some of the concerns I have as a Deputy Prime Minister and Minister of Science and ICT of Korea. The global AI technology competition driven by the United States and China is accelerating.

Korea is aiming to become one of the top three AI nations by strengthening its AI infrastructure,  pushing for national AI transformation, and building a global AI inclusive society.

And what is your evaluation of Korea's AI policies and which key areas should the country focus on going forward? Yeah. I'd like a verse from Professor Yann LeCun.

Speaker 5:
First of all, I want to say I'm extremely impressed that the Korean government chose an AI scientist as the Deputy Prime Minister. This is a unique example in the world,

 and that demonstrates really the importance that I think the Korean government gives to technology in general,  but to AI in particular. The UAE has been a pioneer in having a minister of AI dedicated to that,

 and it looked a little bit odd at first,  but it turned out to be ahead of its time. So I think, I mean, certainly we We're here because Korea has a number of,  you know, top research universities, and in fact, you know,

 Hyunyoung and I co-direct the Global AI Frontier Lab,  which is a joint initiative between ATP and NYU,  and we're really happy with that collaboration for that reason. I think, you know, certainly The tradition is there.

I think ultimately the way Korea and a number of other regions in the world,  such as the EU, Europe more generally, can sort of improve its impact is through international collaboration.

With, you know, starting in the context of academic collaboration, but also industry. And perhaps the angle is to foster, as we were discussing before, collaboration on open source foundation models,

 open source platforms for AI, which in my opinion will as I said previously,  the reason being that if we still Future AI systems will thrive through training on more data and more diverse data.

No single entity, be it Chinese or Californian,  can have access to all the data in the world and all the human knowledge in the world,  right?

AI is eventually going to constitute a repository of all human knowledge that a lot of people will use as a resource to access knowledge wherever it comes from.

And people will need to have access to a wide diversity of AI systems that understand all the languages in the world,  all the cultures, all the value systems, all the centers of interest. No single entity can build a system of this type.

First of all, because it can't have access to all the data. Just having a LLM speak all the official languages of India or Indonesia is practically impossible unless you have You know,

 some collaboration with local entities that can actually collect or digitize that data and perhaps bring it up to quality.

So that's why I think that open source platform will eventually win is because they will have access to more data than the commercial closed source entities. And that's the future I'd like to build.

Speaker 2:
OK, thank you. I hope we make a good collaboration between the Global AI Frontier Lab and the National AI Research Lab in Korea. And next, Professor Yejin Choi, what is your evaluation of Korea AI policies,

 and what areas do you think should be prioritized in the future?

Speaker 4:
So I believe that you have to satisfy three important pillars, two of which I already talked about,  GPUs and talent. You know, there will be some limit as to how many GPUs we may be able to prepare in Korea,

 but still there should be some amount of it available. But you'll be surprised how much culture and the mindset. Could really bring the best out of people and limited resources.

When you have limited resources and limited talent, That one last pillar, which is mindset and culture,  that can really, really go far. And I really mean it.

And if you don't actively think about it, probably that means that there's something you can definitely improve. So don't just assume that everything is great. In the way that we do here, you know, in Korea.

So what I mean by mindset and culture is that, you know,  like I work with so many different students depending on their own mindset about whether they believe that they can do things differently and put really their,

 you know, creative mind into that and then innovate. It has a lot to do with their confidence level. And also, you know, the belief that they can really do it. You have to somehow inject that genuine confidence to people. How do you do that?

Definitely not by making culture, you know, authoritarian, where, you know,  it's all about who's in power and going with the power conservation.

That's not how you really inject confidence to these young, rising students who could really achieve a lot. So culture is another angle in which Any organization,

 it's very easy to have two different groups competing with each other about the same project,  competing essentially for resources. This can also happen across different companies.

We really need to rethink about how we control this because The moment we really collaborate and reduce our ego,  Try to be less selfish, but really think for the country and also for the humanity,

 not just for the country, but also for the global humanity. And then try to do the right thing as opposed to try to maximize the political standing of a particular person or group.

By changing that, we can really, really get the best out of that limited resources we might have. This is really how I made my own career. for a long time, so I could really vouch for this.

Speaker 2:
Okay, thank you. I expect your fully support to grow AI talent in Korea. Next. Professor Cho Kyunghyun, in your opinion, what should Korea focus on to strengthen It's competitiveness in the AI period.

Speaker 3:
I'll just follow up from what Yejin said. As we see today and then in many of the events as well as the things that are organized by the Korean government as well as the companies,  Korea is actually amazing.

Everything works Perfectly everything works, you know, to the T. Everything is perfectly there, right? And that's because we have a lot of the protocols that have been written down and everyone is ...trained and also kind of,

 I say, very encouraged to follow them very, very carefully. But most of those protocols, in my opinion, in particular when it comes to,

 let's say, encouraging the scientific innovations and whatnot, are often the accumulation of the historical,  let's say, efforts that have been made. And then often these historical, let's say,

 packages that come together with the protocols tend to Prevent people from actually being innovative and then trying to do something different. These protocols actually prevent students as well as the researchers to think out of box.

We're actually told to think inside the box. So in my view,  what really needs to be done, in particular in this kind of area where we think that the technology is changing so fast and will have a huge impact.

We need to be able to have a good trust in the scientists who actually know what they're doing. And I'm trying to give them as much flexibility and support as possible so that they can actually do something.

Really innovative rather than just following the protocols that are written down. And there are more of historical baggage rather than something that actually helped them do a good science. That's probably what we need.

Speaker 2:
Thank you. Thank you all of your insight, comment, and based on today's discussion,  we will be able to further enhance Korea AI privacy. And if there is any question from guest, I got two or three questions from you. Is there a question?

Speaker 3:
You didn't think outside box, right? And I guess the out of the protocol, so please. Yes, they didn't prepare the mic.

Speaker 2:
Hi, my name is Dr. Eunhyuk Cha from Gwangju Institute of Science and Technology. I'm really impressed by the very insightful conversations. But then, as Professor Choi said, and also Professor Cho said,

 The current state of the collaborations on AI seems to be very limited,  very... Kind of inconsequential. Compared towards what's happening. And especially when Dr. LeCun said about what has changed with the Ukrainian war,

 that everybody is now jumping over into the autonomous or semi-autonomous means of war without any reservations. The world seems to be going into a very different and difficult situation.

What hasn't been talked about today seems to be about the capital and the roles of the capital. I mean, it appears now that everybody's jumping into this, but behind the scenes,

 the world's top monies, the smart monies and everybody's Pouring unlimited resources into this,

 are there any efforts that you are aware of where capital with better purposes are making efforts to promote and support development and collaboration? So, okay, that's my question.

Speaker 5:
Thank you.

Speaker 2:
How about answering the question of Professor Chen?

Speaker 5:
Yep, cool.

Speaker 4:
So I couldn't hear the question clearly. But I will try to answer your question anyways. I think I heard some keywords. So about maybe the question was about the resources. Who's going to invest? Whoever can invest, please do. I'm not kidding.

Perhaps let me say a few words about what I have seen in other countries. You know, any one country, by the way, can do better. Let me start with that. I have a former student, Antoine Bosselet, who is now a professor at EPFL.

He's swimming in H100s. The government one day decided to buy 10,000s of H100 immediately after ChatGPT, and so,  just like he's achieving so much, And innovating so much.

It's just incredible watching his growth and the growth of Switzerland's AI. Doesn't mean that, you know, you know, like where I am currently, there's GPUs falling from sky yet.

However, however, I see in the case of the US, there are Private foundation funding. Coming from, let's just say, people like Eric Schmidt. Or Bill Gates.

Or, you know, you could name different names, but they just basically make donations out of their pocket. More recently, Andy Konwinski, one of the co-founders of Perplexity, as well as Databricks,

 started this Lode Institute, through which he's going to put his own $100 million to promote academic research. This is quite something to just witness. His law institute wants to be very opinionated about what topic he wants to fund.

The topics currently, there are four topics that he deeply cares about. One is Civic discourse. How do we make sure that we humanity Right now,  political tension grows and people make evil out of each other.

It's a political party all the time. This is a common phenomenon in every country. But is there by chance... Any way to somehow mitigate this through the power of AI. Bring more peace to each other's opinions about each other.

So that's a civic discourse. Another topic is about workforce, future workforce. So basically, it's using AI to really retool people to bring educational power to people at large,  especially in the event where people lose their jobs.

Can we somehow prepare people better in advance? The other domain topic is AI for science. I'm personally very very excited about AI for science where we use AI for Material sciences,  chemistry, medicine, health, drug discovery, you know,

 경험처 is really among the world-class leaders starting way earlier than you know how late I woke up to it but so AI for science is another domain. And then lastly, I'm blanking on, what was the last one? Oh, frontier healthcare.

So frontier healthcare, you know,  making better use of AI because doctors I don't know in Korea maybe situation is better but in America They're overworked and not easily available,

 and I'm having a hard time booking appointments with the doctors around where I live,  so frontier healthcare is another domain.

But just seeing this sort of like innovative ways of trying to shape the academic research to serve the betterment of humanity,  I really find it beautiful. And, you know, whoever is capable of invest, please do.

Speaker 5:
I think it's very important to remember that a lot of innovations that we see,  of course, have been produced by industry but have their roots in academia 5, 10, 15, 20 years earlier.

It took very long for deep learning to come to the fore, right? And it was the product of academic research,  also research in ambitious Industry research lab like Bell Labs who basically don't exist very much anymore.

But then it takes a number of years before people realize, you know, how to scale up,  apply the technology and have an impact, right?

So even though the first convolutional nets were devised in the late 1980s, the revolution in industry only occurred around 2013,  14, 15. It went faster for natural language processing, but there were, you know,

 historically pioneering work by Yoshua Bengio and Renaud Colbert,  Jason Weston, 10 years before natural language processing using deep learning really became important.

There's always a long history, and what's happening today in the U.S. is that people are completely forgetting about this. They think innovation comes from industry.

They think it comes from investing money and hiring engineers and more GPUs,  and they forget that all the ideas come from academia. Probably the best example, actually, is one of Pyongyang's most famous work on attention, right?

The LLMs are built on transformers. The original paper for transformers was called Attention is All You Need. And the attention paper was one where Kun-Yung was a co-author with Yoshua Bengio and Mitri Badanow in 2015, I believe.

And it's really revolutionized natural language processing and now other domains as well in AI. So there's always a long history. So if you invest In research, academic research,

 and then basically create the environment for a good ecosystem of industry research,  but also startups. Around the academic environment, that's how you win. And there is also a question of resources.

So, as I said, Yejin, Switzerland is doing it all right. They are, you know, pushing professors and talented people like Rémi Bosselieu, whom I know very well,

 from various parts of the world, attracting them with, you know, research funding and good salaries and GPUs. I'm really glad that my own home institution, NYU, is also actually providing high-performance computing resources.

I think we can thank Juan for that, in part. Inaugurated a new cluster with 500 GPUs. But what's happening in the U.S. is that the U.S. government is cutting funding for academic research.

There is no national GPU resources for AI researchers. There are in various other parts of the world. Switzerland is one. The European Union is also doing something about it. The U.S. is basically checking out.

They think that all innovation comes from private investment in industry. And I think it's a huge mistake.

Speaker 2:
Thank you. It's time to finish. I would like to sincerely thank Professor Yann LeCun and Professor Yejin Cho. Professor Cho Kyung-hyun and Professor Kim Ki-eun. Taking the time to join us today. Discussions we had were truly valuable.

I hope this session has been meaningful for everyone and I'd like to conclude today's roundtable. Thank you all once again.

Speaker 1:
Thank you so much, Minister Kyung Moon Bae, for your leadership. And I would like to thank our panelists as well.

And I hope it has been a truly fruitful discussion to exchange diverse perspectives as well as inspiring ideas for the future of AI. And with that, we would like to conclude today's morning program.

For those of you who are standing on the stage, you can now return to your seats. The afternoon presentations will begin at 1.30, featuring four different topics, and we look forward to your continued participation.

Please enjoy your lunch, and we'll see you again at 1.30. Thank you.

Speaker 2:
Yeah.

